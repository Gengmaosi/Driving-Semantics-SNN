{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processed data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np_all_train = np.load(r'\\np_all_train_new.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import sampler, Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TopicData(Dataset):\n",
    "    def  __init__(self,inputs,labels):\n",
    "        self.inputs=torch.DoubleTensor(inputs.astype(float)).to(torch.float32)\n",
    "        self.labels=torch.DoubleTensor(labels.astype(float)).to(torch.float32)\n",
    "        self.len = inputs.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        inp = self.inputs[index,:,:]\n",
    "        label = self.labels[index,:,:]\n",
    "        return inp,label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "x = np_all_train[:,:14,:]\n",
    "y = np_all_train[:,14:,:]\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(np_all_train))\n",
    "train_indices = shuffled_indices[:int(len(np_all_train)*0.8)]\n",
    "val_indices = shuffled_indices[int(len(np_all_train)*0.8):int(len(np_all_train)*0.9)]\n",
    "test_indices = shuffled_indices[int(len(np_all_train)*0.9):]\n",
    "\n",
    "x_train = x[train_indices,:,:]\n",
    "y_train = y[train_indices,:,:]\n",
    "x_val = x[val_indices,:,:]\n",
    "y_val = y[val_indices,:,:]\n",
    "x_test = x[test_indices,:,:]\n",
    "y_test = y[test_indices,:,:]\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "trainset = TopicData(x_train, y_train)\n",
    "train_data_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n",
    "valset = TopicData(x_val, y_val)\n",
    "val_data_loader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "testset = TopicData(x_test, y_test)\n",
    "test_data_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from spikingjelly.clock_driven import neuron, encoding, functional\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define and initialize the network\n",
    "tau=2.0\n",
    "num_topic = 5\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(14*num_topic, 256, bias=False),\n",
    "    nn.Linear(256, 512, bias=False),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.Linear(512, 256, bias=False),\n",
    "    nn.Linear(256, num_topic, bias=False),\n",
    "    neuron.LIFNode(tau=tau)\n",
    ")\n",
    "model = model.to('cpu')\n",
    "encoder = encoding.PoissonEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "device = 'cpu'\n",
    "train_epoch = 200\n",
    "T = 50\n",
    "train_times = 0\n",
    "min_test_loss = 10000\n",
    "log_interval = 100\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    print(\"Training...\")\n",
    "    train_correct_sum = 0\n",
    "    train_sum = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run for T timesï¼Œout_spikes_counter is a tensor with shape=[batch_size, 10]\n",
    "        # record the pulse times of 10 neurons in the output layer during the simulation\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                out_spikes_counter = model(encoder(inputs).float())\n",
    "            else:\n",
    "                out_spikes_counter += model(encoder(inputs).float())\n",
    "        \n",
    "        # out_spikes_counter / T obtained the pulse frequency of 10 neurons in the output layer during the simulation\n",
    "        out_spikes_counter_frequency = out_spikes_counter / T\n",
    "        #out_spikes_counter_frequency += model(encoder(inputs).float())\n",
    "        \n",
    "        loss = F.mse_loss(out_spikes_counter_frequency, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # reset the network because the neurons of SNN has memories\n",
    "        functional.reset_net(model)\n",
    "        \n",
    "        train_correct_sum += (out_spikes_counter_frequency.max(1)[1] == labels.max(1)[1]).float().sum().item()\n",
    "        train_sum += inputs.shape[0]\n",
    "        \n",
    "        train_loss += loss.item()*inputs.shape[0]\n",
    "        \n",
    "        train_batch_accuracy = (out_spikes_counter_frequency.max(1)[1] == labels.max(1)[1]).float().mean().item()\n",
    "        if train_times%log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Train Loss : {:.4f} Train batch Accuracy: {:.4f}'.format(\n",
    "                                epoch,batch_idx*inputs.shape[0],len(x_train),batch_idx*inputs.shape[0]/len(trainset)*100,loss.item(),train_batch_accuracy*100))\n",
    "        #writer.add_scalar('train_batch_accuracy', train_batch_accuracy, train_times)\n",
    "        train_accs.append(train_batch_accuracy)\n",
    "        \n",
    "        train_times += 1\n",
    "    train_accuracy = train_correct_sum / train_sum\n",
    "    train_loss = train_loss/len(trainset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sum = 0\n",
    "        correct_sum = 0\n",
    "        test_loss = 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze(1).to(device)\n",
    "            for t in range(T):\n",
    "                if t == 0:\n",
    "                    out_spikes_counter = model(encoder(inputs).float())\n",
    "                else:\n",
    "                    out_spikes_counter += model(encoder(inputs).float())\n",
    "                    \n",
    "            out_spikes_counter = out_spikes_counter / T\n",
    "            correct_sum += (out_spikes_counter.max(1)[1] == labels.max(1)[1]).float().sum().item()\n",
    "            loss = F.mse_loss(out_spikes_counter, labels)\n",
    "            test_loss += loss.item()*inputs.shape[0]\n",
    "            test_sum += inputs.shape[0]\n",
    "            functional.reset_net(model)\n",
    "        test_accuracy = correct_sum / test_sum\n",
    "        test_loss = test_loss/len(valset)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_accuracy)\n",
    "        min_test_loss = min(min_test_loss, test_loss)\n",
    "        if min_test_loss == test_loss:\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), r'\\SNN_topic_new(%d).tar' %(epoch+1))\n",
    "    print(\"Epoch {}: train_loss={}, test_loss={}, test_accuracy={}, min_test_loss={}, train_times={}\".format(\n",
    "                epoch,train_loss,test_loss,test_accuracy,min_test_loss,train_times))\n",
    "    print()\n",
    "    \n",
    "torch.save(model.state_dict(), r'\\SNN_topic_new(Final).tar')\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "torch.save(model.state_dict(), r'\\SNN_topic_new.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
