{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def texts_to_corpus(temp_text):\n",
    "    uni_words = np.unique(np.array(temp_text))\n",
    "    corpus = []\n",
    "    for uni_word in uni_words:\n",
    "        corpus.append((uni_word,temp_text.count(uni_word)))\n",
    "    return corpus\n",
    "\n",
    "def test2str(t_texts):\n",
    "    str_text = []\n",
    "    for t_text in t_texts:\n",
    "        str_text.append(str(t_text))\n",
    "    return str_text\n",
    "\n",
    "third_stage = pd.read_csv(r'\\clean_1Hz(new).csv')\n",
    "\n",
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "\n",
    "texts = []\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    temp_words = single_data['words'].values\n",
    "    #print(temp_words)\n",
    "    temp_text = []\n",
    "    for temp_word in temp_words:\n",
    "        temp_text.append(str(temp_word))\n",
    "    #print(temp_text)\n",
    "    texts.append(temp_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "#import jieba.posseg as jp, jieba\n",
    "\n",
    "dictionary = corpora.Dictionary.load(r'\\dic_new.dict')\n",
    "corpus = [dictionary.doc2bow(words) for words in texts]\n",
    "# print(len(corpus))\n",
    "# print(dictionary,dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "print(len(corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import models, corpora\n",
    "\n",
    "ldamodel = models.ldamodel.LdaModel.load(r\"\\auto_model_new_test.model\")\n",
    "\n",
    "#test the effectiveness of ldamodel\n",
    "\n",
    "test_cor = dictionary.doc2bow(['683','528','501','714','497','714','683'])\n",
    "#test_cor = [words_to_cor(683),words_to_cor(528),words_to_cor(501),words_to_cor(714),words_to_cor(497),words_to_cor(714),words_to_cor(683)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['657','831','840','863','687','869','807'])\n",
    "#test_cor = [words_to_cor(657),words_to_cor(831),words_to_cor(840),words_to_cor(863),words_to_cor(687),words_to_cor(869),words_to_cor(807)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['499','683','714','684','558','528','528'])\n",
    "#test_cor = [words_to_cor(499),words_to_cor(683),words_to_cor(714),words_to_cor(684),words_to_cor(558),words_to_cor(528),words_to_cor(528)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['373','404','404','373','404','373','372'])\n",
    "#test_cor = [words_to_cor(373),words_to_cor(404),words_to_cor(404),words_to_cor(373),words_to_cor(404),words_to_cor(373),words_to_cor(372)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "#test_cor = [(528, 6), (558, 1)]\n",
    "test_cor = dictionary.doc2bow(['528','528','528','528','528','528','558'])\n",
    "#test_cor = [words_to_cor(373),words_to_cor(404),words_to_cor(404),words_to_cor(373),words_to_cor(404),words_to_cor(373),words_to_cor(372)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-quality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_known(prediction):\n",
    "    known_topics = []\n",
    "    knowm_fea = []\n",
    "    for i in range(len(prediction)):\n",
    "        known_topics.append(prediction[i][0])\n",
    "        knowm_fea.append(prediction[i][1])\n",
    "    return known_topics,knowm_fea\n",
    "\n",
    "def construct_feature(prediction,num_topic):\n",
    "    known_topics,knowm_fea = search_known(prediction)\n",
    "    #print(known_topics)\n",
    "    temp_feature = []\n",
    "    if len(known_topics)==num_topic:\n",
    "        return knowm_fea\n",
    "    for i in range(num_topic):\n",
    "        completion = (1-sum(knowm_fea))/(num_topic-len(known_topics))\n",
    "        if i in known_topics:\n",
    "            indexs = known_topics.index(i)\n",
    "            temp_feature.append(knowm_fea[indexs])\n",
    "        else:\n",
    "            temp_feature.append(completion)\n",
    "    return temp_feature\n",
    "\n",
    "def test2str(t_texts):\n",
    "    str_text = []\n",
    "    for t_text in t_texts:\n",
    "        str_text.append(str(t_text))\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-step data preprocess\n",
    "\n",
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "total_times = 0\n",
    "num_topic = 5\n",
    "train_data = []\n",
    "single_length = []\n",
    "\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    for i in range(len(single_data)-6):\n",
    "        #for j in range(i,i+7):\n",
    "        temp_text = [single_data.loc[i]['words'],single_data.loc[i+1]['words'],single_data.loc[i+2]['words'],\n",
    "                    single_data.loc[i+3]['words'],single_data.loc[i+4]['words'],single_data.loc[i+5]['words'],\n",
    "                    single_data.loc[i+6]['words']]\n",
    "        \n",
    "        tuple_corpus = dictionary.doc2bow(test2str(temp_text))\n",
    "        temp_feature = construct_feature(ldamodel[tfidf_model[tuple_corpus]],num_topic)\n",
    "        ID = single_data.loc[i+6]['id']\n",
    "        t = single_data.loc[i+6]['t']\n",
    "        corpus = dictionary.doc2bow([str(single_data.loc[i+6]['words'])])\n",
    "        his_text = [single_data.loc[i+1]['words'],single_data.loc[i+2]['words'],\n",
    "                    single_data.loc[i+3]['words'],single_data.loc[i+4]['words'],single_data.loc[i+5]['words'],\n",
    "                    single_data.loc[i+6]['words']]\n",
    "        train_data.append([ID,t,len(single_data)-6,corpus,temp_feature,int(single_data.loc[i+6]['words']),his_text])\n",
    "    total_times += len(single_data)-6\n",
    "    single_length.append(len(single_data)-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(temp_train):\n",
    "    np_train = np.array(temp_train)\n",
    "    if np_train.shape[0]>14:\n",
    "        print('error')\n",
    "        return np_train\n",
    "    elif np_train.shape[0]==14:\n",
    "        return np_train\n",
    "    else:\n",
    "        loss_len = 14-np_train.shape[0]\n",
    "        loss_train = np.zeros([loss_len,5])\n",
    "        return np.concatenate([loss_train,np_train],axis=0)\n",
    "\n",
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "flag = 0\n",
    "minimum = 4\n",
    "maximum = 14\n",
    "all_train = []\n",
    "train_speed = []\n",
    "train_topic = []\n",
    "history_speed = []\n",
    "np_all_train = np.zeros([0,14,5])\n",
    "\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    length = single_length[int(ID)-1]\n",
    "    temp_data = train_data[flag:flag+length]\n",
    "    flag += length\n",
    "    if length<4+15:\n",
    "        continue\n",
    "    for i in range(length-minimum+1-15):\n",
    "        end_num = min(length-15,i+maximum)\n",
    "        #print(i,end_num)\n",
    "        for j in range(i+minimum-1,end_num):\n",
    "            next_speed = [temp_data[j+1][5],temp_data[j+2][5],temp_data[j+3][5],temp_data[j+4][5],temp_data[j+5][5],temp_data[j+6][5],temp_data[j+7][5],temp_data[j+8][5],temp_data[j+9][5],temp_data[j+10][5],temp_data[j+11][5],temp_data[j+12][5],temp_data[j+13][5],temp_data[j+14][5],temp_data[j+15][5]]\n",
    "            next_topic = [temp_data[j+1][4],temp_data[j+2][4],temp_data[j+3][4],temp_data[j+4][4],temp_data[j+5][4],temp_data[j+6][4],temp_data[j+7][4],temp_data[j+8][4],temp_data[j+9][4],temp_data[j+10][4],temp_data[j+11][4],temp_data[j+12][4],temp_data[j+13][4],temp_data[j+14][4],temp_data[j+15][4]]\n",
    "            train_speed.append(next_speed)\n",
    "            train_topic.append(next_topic)\n",
    "            history_speed.append(temp_data[j][6])\n",
    "            temp_train = []\n",
    "            for k in range(i,j+1):\n",
    "                temp_train.append(temp_data[k][4])\n",
    "            np_temp_train = pad_seq(temp_train).reshape([-1,pad_seq(temp_train).shape[0],pad_seq(temp_train).shape[1]])\n",
    "            all_train.append(temp_train)\n",
    "            np_all_train = np.concatenate([np_all_train,np_temp_train],axis=0)\n",
    "#print(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_speed = np.array(train_speed)\n",
    "np_train_topic = np.array(train_topic)\n",
    "#np_history_speed = np.array(history_speed)\n",
    "print(np_all_train.shape)\n",
    "print(np_train_speed.shape,np_train_topic.shape)\n",
    "\n",
    "np_history_speed = np.zeros([len(history_speed),6])\n",
    "for i in range(len(history_speed)):\n",
    "    for j in range(len(history_speed[i])):\n",
    "        np_history_speed[i,j] = history_speed[i][j]\n",
    "        \n",
    "np.save(r'\\np_train_speed_multistep_new.npy',np_train_speed)\n",
    "np.save(r'\\np_train_topic_multistep_new.npy',np_train_topic)\n",
    "np.save(r'\\np_all_train_multistep_new.npy',np_all_train)\n",
    "np.save(r'\\np_history_speed_multistep_new.npy',np_history_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-table",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-step topic prediction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import sampler, Dataset, DataLoader\n",
    "\n",
    "class MultiTopicData(Dataset):\n",
    "    def  __init__(self,inputs,labels):\n",
    "        self.inputs=torch.DoubleTensor(inputs.astype(float)).to(torch.float32)\n",
    "        self.labels=torch.DoubleTensor(labels.astype(float)).to(torch.float32)\n",
    "        \n",
    "        self.len = inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        inp = self.inputs[index,:,:]\n",
    "        label = self.labels[index,:,:]\n",
    "        \n",
    "        return inp,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "x = np_all_train\n",
    "y = np_train_topic\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(np_all_train))\n",
    "#train_indices = shuffled_indices[:int(len(np_all_train)*0.8)]\n",
    "#val_indices = shuffled_indices[int(len(np_all_train)*0.8):int(len(np_all_train)*0.9)]\n",
    "test_indices = shuffled_indices[int(len(np_all_train)*0.90):]\n",
    "# test_indices_1 = shuffled_indices[int(len(np_all_train)*0.90):int(len(np_all_train)*0.95)]\n",
    "# test_indices_2 = shuffled_indices[int(len(np_all_train)*0.95):]\n",
    "\n",
    "#x_train = x[train_indices,:,:]\n",
    "#y_train = y[train_indices,:]\n",
    "#x_val = x[val_indices,:,:]\n",
    "#y_val = y[val_indices,:]\n",
    "x_test = x[test_indices,:,:]\n",
    "y_test = y[test_indices,:]\n",
    "# x_test_1 = x[test_indices_1,:,:]\n",
    "# y_test_1 = y[test_indices_1,:]\n",
    "# x_test_2 = x[test_indices_2,:,:]\n",
    "# y_test_2 = y[test_indices_2,:]\n",
    "\n",
    "#print(x_train.shape,y_train.shape)\n",
    "#print(x_val.shape,y_val.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "# print(x_test_1.shape,y_test_1.shape)\n",
    "# print(x_test_2.shape,y_test_2.shape)\n",
    "\n",
    "#trainset = TopicData(x_train, y_train)\n",
    "#train_data_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n",
    "#valset = TopicData(x_val, y_val)\n",
    "#val_data_loader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "topic_testset = MultiTopicData(x_test,y_test)\n",
    "topic_test_data_loader = DataLoader(topic_testset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# topic_testset_1 = MultiTopicData(x_test_1,y_test_1)\n",
    "# topic_test_data_loader_1 = DataLoader(topic_testset_1, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "# topic_testset_2 = MultiTopicData(x_test_2,y_test_2)\n",
    "# topic_test_data_loader_2 = DataLoader(topic_testset_2, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from spikingjelly.clock_driven import neuron, encoding, functional\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define and initialize the network\n",
    "tau=2.0\n",
    "num_topic = 5\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(14*num_topic, 256, bias=False),\n",
    "    nn.Linear(256, 512, bias=False),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.Linear(512, 256, bias=False),\n",
    "    nn.Linear(256, num_topic, bias=False),\n",
    "    neuron.LIFNode(tau=tau)\n",
    ")\n",
    "model = model.to('cpu')\n",
    "encoder = encoding.PoissonEncoder()\n",
    "\n",
    "model.load_state_dict(torch.load(r'\\SNN_topic_new.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi step topic prediction\n",
    "test_sum = [0,0,0,0,0,0,0,0,0,0]\n",
    "correct_sum = [0,0,0,0,0,0,0,0,0,0]\n",
    "test_loss = [0,0,0,0,0,0,0,0,0,0]\n",
    "T = 50\n",
    "all_outputs = torch.zeros([0,10,5])\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(topic_test_data_loader):\n",
    "    #inputs = inputs.to(device)\n",
    "    #labels = labels.to(device)\n",
    "    if batch_idx%10==0:\n",
    "        print(batch_idx*inputs.shape[0]/len(x_test)*100)\n",
    "    \n",
    "    all_inputs = inputs\n",
    "    time_step = 0\n",
    "    for time_step in range(10):\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                out_spikes_counter = model(encoder(all_inputs[:,-14:,:]).float())\n",
    "            else:\n",
    "                out_spikes_counter += model(encoder(all_inputs[:,-14:,:]).float())\n",
    "        \n",
    "        out_spikes_counter = out_spikes_counter / T\n",
    "        correct_sum[time_step] += (out_spikes_counter.max(1)[1] == labels[:,time_step,:].max(1)[1]).float().sum().item()\n",
    "        loss = F.mse_loss(out_spikes_counter, labels[:,time_step,:])\n",
    "        test_loss[time_step] += loss.item()*inputs.shape[0]\n",
    "        test_sum[time_step] += inputs.shape[0]\n",
    "        \n",
    "        all_inputs = torch.cat([all_inputs,out_spikes_counter.unsqueeze(1)],dim=1)\n",
    "    all_outputs = torch.cat([all_outputs,all_inputs[:,-15:,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    print('{}s top 1 topic accuracy: {:.2f}%'.format(t+1,correct_sum[t]/test_sum[t]*100))\n",
    "for t in range(10):\n",
    "    print('{}s RMSE Loss: {:.2f}'.format(t+1,np.sqrt(test_loss[t]/test_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-press",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-step speed prediction\n",
    "\n",
    "bow_train_speeds = np.zeros([np_train_speed.shape[0],np_train_speed.shape[1]],dtype=int)\n",
    "#bow_history_speeds = np.zeros([np_train_speed.shape[0],np_train_speed.shape[1]],dtype=int)\n",
    "\n",
    "for idx,temp_speed in enumerate(np_train_speed):\n",
    "    #print(temp_speed)\n",
    "    temp_bow_train_speed = []\n",
    "    for each_speed in test2str(temp_speed):\n",
    "        temp_bow_train_speed.append(int(dictionary.doc2bow([each_speed])[0][0]))\n",
    "    bow_train_speeds[[idx],:] = np.array(temp_bow_train_speed,dtype=int).reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import sampler, Dataset, DataLoader\n",
    "\n",
    "class MultiSpeedData(Dataset):\n",
    "    def  __init__(self,inputs,inputs_speed,labels):\n",
    "        self.inputs=torch.DoubleTensor(inputs.astype(float)).to(torch.float32)\n",
    "        self.inputs_speed=torch.DoubleTensor(inputs_speed.astype(float)).to(torch.float32)\n",
    "        self.labels=torch.DoubleTensor(labels.astype(float)).to(torch.float32)\n",
    "        \n",
    "        self.len = inputs.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        inp = self.inputs[index,:,:]\n",
    "        input_speed = self.inputs_speed[index,:]\n",
    "        label = self.labels[index,:]\n",
    "        \n",
    "        return inp,input_speed,label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "x = np_all_train\n",
    "x_speed = np_history_speed\n",
    "y = bow_train_speeds\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(np_all_train))\n",
    "# train_indices = shuffled_indices[:int(len(np_all_train)*0.8)]\n",
    "# val_indices = shuffled_indices[int(len(np_all_train)*0.8):int(len(np_all_train)*0.9)]\n",
    "test_indices = shuffled_indices[int(len(np_all_train)*0.9):]\n",
    "# test_indices_2 = shuffled_indices[int(len(np_all_train)*0.95):]\n",
    "# test_indices_1 = shuffled_indices[int(len(np_all_train)*0.90):int(len(np_all_train)*0.95)]\n",
    "\n",
    "# x_train = x[train_indices,:,:]\n",
    "# y_train = y[train_indices,:]\n",
    "# x_val = x[val_indices,:,:]\n",
    "# y_val = y[val_indices,:]\n",
    "x_test = x[test_indices,:,:]\n",
    "x_speed_test = x_speed[test_indices,:]\n",
    "y_test = y[test_indices,:]\n",
    "# x_test_1 = x[test_indices_1,:,:]\n",
    "# x_speed_test_1 = x_speed[test_indices_1,:]\n",
    "# y_test_1 = y[test_indices_1,:]\n",
    "# x_test_2 = x[test_indices_2,:,:]\n",
    "# x_speed_test_2 = x_speed[test_indices_2,:]\n",
    "# y_test_2 = y[test_indices_2,:]\n",
    "\n",
    "#print(x_train.shape,y_train.shape)\n",
    "#print(x_val.shape,y_val.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "# print(x_test_1.shape,y_test_1.shape)\n",
    "# print(x_test_2.shape,y_test_2.shape)\n",
    "\n",
    "#trainset = TopicData(x_train, y_train)\n",
    "#train_data_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n",
    "#valset = TopicData(x_val, y_val)\n",
    "#val_data_loader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "testset = MultiSpeedData(x_test,x_speed_test,y_test)\n",
    "test_data_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n",
    "# testset_1 = MultiSpeedData(x_test_1,x_speed_test_1,y_test_1)\n",
    "# test_data_loader_1 = DataLoader(testset_1, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n",
    "# testset_2 = MultiSpeedData(x_test_2,x_speed_test_2,y_test_2)\n",
    "# test_data_loader_2 = DataLoader(testset_2, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "def array_to_cor(inputs_speed):\n",
    "    array_speed = inputs_speed.cpu().numpy()\n",
    "    cor_speed = []\n",
    "    for i in range(6):\n",
    "        cor_speed.append((int(array_speed[0,2*i+0]),int(array_speed[0,2*i+1])))\n",
    "    return cor_speed\n",
    "\n",
    "def words_to_cor(words):\n",
    "    a = math.ceil(words/30)\n",
    "    b = words % 30\n",
    "    return (a,b)\n",
    "\n",
    "def cor_to_words(cor):\n",
    "    a,b = cor\n",
    "    words = (a-1)*30.0+b\n",
    "    return words\n",
    "\n",
    "def lon_speed(lon):\n",
    "    if lon != 6:\n",
    "        return 2+(lon-1)*4\n",
    "    else:\n",
    "        return 22\n",
    "\n",
    "def lat_speed(lat):\n",
    "    if lat == 1:\n",
    "        return -0.4479\n",
    "    elif lat == 2:\n",
    "        return -0.2240\n",
    "    elif lat == 3:\n",
    "        return 0\n",
    "    elif lat == 4:\n",
    "        return 0.2240\n",
    "    elif lat == 5:\n",
    "        return 0.4479\n",
    "    \n",
    "def cor_to_speed(cor):\n",
    "    a,b = cor\n",
    "    a_lon = math.ceil(a/5)\n",
    "    a_lat = a - (a_lon-1)*5\n",
    "    b_lon = math.ceil(b/5)\n",
    "    b_lat = b - (b_lon-1)*5\n",
    "    \n",
    "    return [a_lon,a_lat,b_lon,b_lat],[lon_speed(a_lon),lon_speed(b_lon)],[lat_speed(a_lat),lat_speed(b_lat)]\n",
    "\n",
    "def batch_speed_trans(outputs):\n",
    "    #outputs = outputs.item()\n",
    "    lons = np.zeros([0,2])\n",
    "    lats = np.zeros([0,2])\n",
    "    for output in outputs:\n",
    "        _,lon_v_pre,lat_v_pre = cor_to_speed(words_to_cor(output))\n",
    "        lons = np.concatenate([lons,np.array(lon_v_pre).reshape(1,-1)],axis=0)\n",
    "        lats = np.concatenate([lats,np.array(lat_v_pre).reshape(1,-1)],axis=0)\n",
    "    return lons,lats\n",
    "\n",
    "def hot2doc(out_spikes_counter_max,dictionary):\n",
    "    docs = []\n",
    "    for hot_code in out_spikes_counter_max:\n",
    "        docs.append(int(dictionary[hot_code.item()]))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from spikingjelly.clock_driven import neuron, encoding, functional\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define and initialize the network\n",
    "tau=5.0\n",
    "num_topic = 5\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(14*num_topic, 256, bias=False),\n",
    "    nn.Linear(256, 512, bias=False),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.Linear(512, 256, bias=False),\n",
    "    nn.Linear(256, 307, bias=False),\n",
    "    neuron.LIFNode(tau=tau)\n",
    ")\n",
    "model = model.to('cpu')\n",
    "encoder = encoding.PoissonEncoder()\n",
    "\n",
    "model.load_state_dict(torch.load(r'\\SNN_speed_new.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi step speed prediction\n",
    "test_sum = 0\n",
    "correct_sum = 0\n",
    "test_loss = 0\n",
    "T = 20\n",
    "num_topic = 5\n",
    "all_outputs_lons = np.zeros([0,30])\n",
    "all_outputs_lats = np.zeros([0,30])\n",
    "all_labels_lons = np.zeros([0,30])\n",
    "all_labels_lats = np.zeros([0,30])\n",
    "for batch_idx, (inputs,inputs_speed,labels) in enumerate(test_data_loader):\n",
    "    if batch_idx%10==0:\n",
    "        print(batch_idx*inputs.shape[0]/len(x_test)*100)\n",
    "    \n",
    "    history_speed = list(np.array(inputs_speed.cpu().numpy()[0],dtype=int))\n",
    "    labels = labels.squeeze(1)\n",
    "    \n",
    "    temp_outputs_lons = np.zeros([1,30])\n",
    "    temp_outputs_lats = np.zeros([1,30])\n",
    "    temp_labels_lons = np.zeros([1,30])\n",
    "    temp_labels_lats = np.zeros([1,30])\n",
    "    \n",
    "    temp_steps = 0\n",
    "    step_inputs = inputs\n",
    "    \n",
    "    for temp_steps in range(10):\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                out_spikes_counter = model(encoder(step_inputs).float())\n",
    "            else:\n",
    "                out_spikes_counter += model(encoder(step_inputs).float())\n",
    "        out_spikes_counter = out_spikes_counter / T\n",
    "    \n",
    "        output_onestep = out_spikes_counter.max(1)[1]\n",
    "        outputs_lons,outputs_lats = batch_speed_trans(hot2doc(output_onestep,dictionary))\n",
    "        labels_lons,labels_lats = batch_speed_trans(hot2doc(labels[0,temp_steps].unsqueeze(0),dictionary))\n",
    "        temp_outputs_lons[:,temp_steps*2:temp_steps*2+2] = outputs_lons\n",
    "        temp_outputs_lats[:,temp_steps*2:temp_steps*2+2] = outputs_lats\n",
    "        temp_labels_lons[:,temp_steps*2:temp_steps*2+2] = labels_lons\n",
    "        temp_labels_lats[:,temp_steps*2:temp_steps*2+2] = labels_lats\n",
    "        \n",
    "        history_speed.append(int(dictionary[output_onestep.item()]))\n",
    "        tuple_corpus = dictionary.doc2bow(test2str(history_speed[-7:]))\n",
    "        temp_feature = construct_feature(ldamodel[tfidf_model[tuple_corpus]],num_topic)\n",
    "        temp_feature = np.array(temp_feature).astype(float)\n",
    "        step_inp = torch.DoubleTensor(temp_feature).to(torch.float32).reshape([1,1,-1])\n",
    "        step_inputs = torch.cat([step_inputs,step_inp],dim=1)[:,-14:,:]\n",
    "    \n",
    "    all_outputs_lons = np.concatenate([all_outputs_lons,temp_outputs_lons],axis=0)\n",
    "    all_outputs_lats = np.concatenate([all_outputs_lats,temp_outputs_lats],axis=0)\n",
    "    all_labels_lons = np.concatenate([all_labels_lons,temp_labels_lons],axis=0)\n",
    "    all_labels_lats = np.concatenate([all_labels_lats,temp_labels_lats],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "for t in range(10):\n",
    "    print('{}s prediction error : '.format(t+1))\n",
    "    print(f\"Lon_v: MSE：{mean_squared_error(all_outputs_lons[:,2*t:2*t+2], all_labels_lons[:,2*t:2*t+2])}\")\n",
    "    print(f\"Lon_v: RMSE：{np.sqrt(mean_squared_error(all_outputs_lons[:,2*t:2*t+2], all_labels_lons[:,2*t:2*t+2]))}\")\n",
    "    print(f\"Lon_v: MAE：{mean_absolute_error(all_outputs_lons[:,2*t:2*t+2], all_labels_lons[:,2*t:2*t+2])}\")\n",
    "    print(f\"Lat_v: MSE：{mean_squared_error(all_outputs_lats[:,2*t:2*t+2], all_labels_lats[:,2*t:2*t+2])}\")\n",
    "    print(f\"Lat_v: RMSE：{np.sqrt(mean_squared_error(all_outputs_lats[:,2*t:2*t+2], all_labels_lats[:,2*t:2*t+2]))}\")\n",
    "    print(f\"Lat_v: MAE：{mean_absolute_error(all_outputs_lats[:,2*t:2*t+2], all_labels_lats[:,2*t:2*t+2])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-uniform",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
