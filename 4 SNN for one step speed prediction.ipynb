{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np_all_train = np.load(r'\\np_all_train_new.npy')\n",
    "np_train_speed = np.load(r'\\np_train_speed_new.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary.load(r'\\dic_new.dict')\n",
    "# print(dictionary,dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2str(t_texts):\n",
    "    str_text = []\n",
    "    for t_text in t_texts:\n",
    "        str_text.append(str(t_text))\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_speeds = []\n",
    "for temp_speed in np_train_speed:\n",
    "    #print(temp_speed)\n",
    "    bow_speeds.append(dictionary.doc2bow(test2str(temp_speed))[0][0])\n",
    "\n",
    "np_bow_speeds = np.array(bow_speeds).reshape(-1,1)\n",
    "# print(np_bow_speeds.shape,np_train_speed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the processed data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import sampler, Dataset, DataLoader\n",
    "\n",
    "class TopicData(Dataset):\n",
    "    def  __init__(self,inputs,labels):\n",
    "        self.inputs=torch.DoubleTensor(inputs.astype(float)).to(torch.float32)\n",
    "        self.labels=torch.DoubleTensor((labels).astype(float)).to(torch.float32)\n",
    "        self.labels_one_hot = F.one_hot(self.labels.to(torch.int64), 307).float()\n",
    "        self.len = inputs.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        inp = self.inputs[index,:,:]\n",
    "        label = self.labels[index,:]\n",
    "        label_one_hot = self.labels_one_hot[index,:,:].squeeze(0)\n",
    "        return inp,label,label_one_hot\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "x = np_all_train[:,:14,:]\n",
    "y = np_bow_speeds\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(np_all_train))\n",
    "train_indices = shuffled_indices[:int(len(np_all_train)*0.8)]\n",
    "val_indices = shuffled_indices[int(len(np_all_train)*0.8):int(len(np_all_train)*0.9)]\n",
    "test_indices = shuffled_indices[int(len(np_all_train)*0.9):]\n",
    "\n",
    "x_train = x[train_indices,:,:]\n",
    "y_train = y[train_indices,:]\n",
    "x_val = x[val_indices,:,:]\n",
    "y_val = y[val_indices,:]\n",
    "x_test = x[test_indices,:,:]\n",
    "y_test = y[test_indices,:]\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "trainset = TopicData(x_train, y_train)\n",
    "train_data_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n",
    "valset = TopicData(x_val, y_val)\n",
    "val_data_loader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)\n",
    "testset = TopicData(x_test, y_test)\n",
    "test_data_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "#307 is the number of words in dictionary\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from spikingjelly.clock_driven import neuron, encoding, functional\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define and initialize the network\n",
    "\n",
    "tau=5.0\n",
    "num_topic = 5\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(14*num_topic, 256, bias=False),\n",
    "    nn.Linear(256, 512, bias=False),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.Linear(512, 256, bias=False),\n",
    "    nn.Linear(256, 307, bias=False),\n",
    "    neuron.LIFNode(tau=tau)\n",
    ")\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "def array_to_cor(inputs_speed):\n",
    "    array_speed = inputs_speed.cpu().numpy()\n",
    "    cor_speed = []\n",
    "    for i in range(6):\n",
    "        cor_speed.append((int(array_speed[0,2*i+0]),int(array_speed[0,2*i+1])))\n",
    "    return cor_speed\n",
    "\n",
    "def words_to_cor(words):\n",
    "    a = math.ceil(words/30)\n",
    "    b = words % 30\n",
    "    return (a,b)\n",
    "\n",
    "def cor_to_words(cor):\n",
    "    a,b = cor\n",
    "    words = (a-1)*30.0+b\n",
    "    return words\n",
    "\n",
    "def lon_speed(lon):\n",
    "    if lon != 6:\n",
    "        return 2+(lon-1)*4\n",
    "    else:\n",
    "        return 22\n",
    "\n",
    "def lat_speed(lat):\n",
    "    if lat == 1:\n",
    "        return -0.4479\n",
    "    elif lat == 2:\n",
    "        return -0.2240\n",
    "    elif lat == 3:\n",
    "        return 0\n",
    "    elif lat == 4:\n",
    "        return 0.2240\n",
    "    elif lat == 5:\n",
    "        return 0.4479\n",
    "    \n",
    "def cor_to_speed(cor):\n",
    "    a,b = cor\n",
    "    a_lon = math.ceil(a/5)\n",
    "    a_lat = a - (a_lon-1)*5\n",
    "    b_lon = math.ceil(b/5)\n",
    "    b_lat = b - (b_lon-1)*5\n",
    "    \n",
    "    return [a_lon,a_lat,b_lon,b_lat],[lon_speed(a_lon),lon_speed(b_lon)],[lat_speed(a_lat),lat_speed(b_lat)]\n",
    "\n",
    "def batch_speed_trans(outputs):\n",
    "    #outputs = outputs.item()\n",
    "    lons = np.zeros([0,2])\n",
    "    lats = np.zeros([0,2])\n",
    "    for output in outputs:\n",
    "        _,lon_v_pre,lat_v_pre = cor_to_speed(words_to_cor(output))\n",
    "        lons = np.concatenate([lons,np.array(lon_v_pre).reshape(1,-1)],axis=0)\n",
    "        lats = np.concatenate([lats,np.array(lat_v_pre).reshape(1,-1)],axis=0)\n",
    "    return lons,lats\n",
    "\n",
    "def hot2doc(out_spikes_counter_max,dictionary):\n",
    "    docs = []\n",
    "    for hot_code in out_spikes_counter_max:\n",
    "        docs.append(int(dictionary[hot_code.item()]))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "device = 'cpu'\n",
    "train_epoch = 100\n",
    "T = 20\n",
    "train_times = 0\n",
    "min_test_loss = 10000\n",
    "log_interval = 100\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "encoder = encoding.PoissonEncoder()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    print(\"Training...\")\n",
    "    train_correct_sum = 0\n",
    "    train_sum = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, labels,labels_one_hot) in enumerate(train_data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.squeeze(1).to(device)\n",
    "        labels_one_hot = labels_one_hot.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # run for T timesï¼Œout_spikes_counter is a tensor with shape=[batch_size, 10]\n",
    "        # record the pulse times of 10 neurons in the output layer during the simulation\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                out_spikes_counter = model(encoder(inputs).float())\n",
    "            else:\n",
    "                out_spikes_counter += model(encoder(inputs).float())\n",
    "\n",
    "        # out_spikes_counter / T obtained the pulse frequency of 10 neurons in the output layer during the simulation\n",
    "        out_spikes_counter_frequency = out_spikes_counter / T\n",
    "\n",
    "        #loss = criterion(out_spikes_counter_frequency.to(torch.float32), labels.long())\n",
    "        loss = F.mse_loss(out_spikes_counter_frequency,labels_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # reset the network because the neurons of SNN has memories\n",
    "        functional.reset_net(model)\n",
    "        \n",
    "        train_correct_sum += (out_spikes_counter_frequency.max(1)[1] == labels).float().sum().item()\n",
    "        train_sum += inputs.shape[0]\n",
    "        \n",
    "        train_loss += loss.item()*inputs.shape[0]\n",
    "        \n",
    "        train_batch_accuracy = (out_spikes_counter_frequency.max(1)[1] == labels).float().mean().item()\n",
    "        if train_times%log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Train Loss : {:.4f} Train batch Accuracy: {:.4f}'.format(\n",
    "                                epoch,batch_idx*inputs.shape[0],len(x_train),batch_idx*inputs.shape[0]/len(trainset)*100,loss.item(),train_batch_accuracy*100))\n",
    "        #writer.add_scalar('train_batch_accuracy', train_batch_accuracy, train_times)\n",
    "        train_accs.append(train_batch_accuracy)\n",
    "\n",
    "        train_times += 1\n",
    "    train_accuracy = train_correct_sum / train_sum\n",
    "    train_loss = train_loss/len(trainset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sum = 0\n",
    "        correct_sum = 0\n",
    "        test_loss = 0\n",
    "        all_outputs_lons = np.zeros([0,2])\n",
    "        all_outputs_lats = np.zeros([0,2])\n",
    "        all_labels_lons = np.zeros([0,2])\n",
    "        all_labels_lats = np.zeros([0,2])\n",
    "        for batch_idx, (inputs, labels,labels_one_hot) in enumerate(val_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze(1).to(device)\n",
    "            labels_one_hot = labels_one_hot.to(device)\n",
    "            for t in range(T):\n",
    "                if t == 0:\n",
    "                    out_spikes_counter = model(encoder(inputs).float())\n",
    "                else:\n",
    "                    out_spikes_counter += model(encoder(inputs).float())\n",
    "                    \n",
    "            out_spikes_counter = out_spikes_counter / T\n",
    "            correct_sum += (out_spikes_counter.max(1)[1] == labels).float().sum().item()\n",
    "            #loss = criterion(out_spikes_counter.to(torch.float32), labels.long())\n",
    "            loss = F.mse_loss(out_spikes_counter,labels_one_hot)\n",
    "            test_loss += loss.item()*inputs.shape[0]\n",
    "            test_sum += inputs.shape[0]\n",
    "            functional.reset_net(model)\n",
    "            outputs_lons,outputs_lats = batch_speed_trans(hot2doc(out_spikes_counter.max(1)[1],dictionary))\n",
    "            labels_lons,labels_lats = batch_speed_trans(hot2doc(labels,dictionary))\n",
    "            all_outputs_lons = np.concatenate([all_outputs_lons,outputs_lons],axis=0)\n",
    "            all_outputs_lats = np.concatenate([all_outputs_lats,outputs_lats],axis=0)\n",
    "            all_labels_lons = np.concatenate([all_labels_lons,labels_lons],axis=0)\n",
    "            all_labels_lats = np.concatenate([all_labels_lats,labels_lats],axis=0)\n",
    "        test_accuracy = correct_sum / test_sum\n",
    "        test_loss = test_loss/len(valset)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_accuracy)\n",
    "        min_test_loss = min(min_test_loss, test_loss)\n",
    "        if min_test_loss == test_loss:\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), r'\\SNN_speed_new(%d).tar' %(epoch+1))\n",
    "    print(\"Epoch {}: train_loss={}, test_loss={}, test_accuracy={}, min_test_loss={}, train_times={}\".format(\n",
    "                epoch,train_loss,test_loss,test_accuracy,min_test_loss,train_times))\n",
    "    print(f\"Lon_v: MSEï¼š{mean_squared_error(outputs_lons, labels_lons)}\")\n",
    "    print(f\"Lon_v: RMSEï¼š{np.sqrt(mean_squared_error(outputs_lons, labels_lons))}\")\n",
    "    print(f\"Lon_v: MAEï¼š{mean_absolute_error(outputs_lons, labels_lons)}\")\n",
    "    print(f\"Lat_v: MSEï¼š{mean_squared_error(outputs_lats, labels_lats)}\")\n",
    "    print(f\"Lat_v: RMSEï¼š{np.sqrt(mean_squared_error(outputs_lats, labels_lats))}\")\n",
    "    print(f\"Lat_v: MAEï¼š{mean_absolute_error(outputs_lats, labels_lats)}\")\n",
    "    print()\n",
    "    \n",
    "torch.save(model.state_dict(), r'\\SNN_speed_new(Final).tar')\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "torch.save(model.state_dict(), r'\\SNN_speed_new.tar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
