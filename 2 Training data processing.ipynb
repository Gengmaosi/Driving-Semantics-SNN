{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process origin data from step 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def texts_to_corpus(temp_text):\n",
    "    uni_words = np.unique(np.array(temp_text))\n",
    "    corpus = []\n",
    "    for uni_word in uni_words:\n",
    "        corpus.append((uni_word,temp_text.count(uni_word)))\n",
    "    return corpus\n",
    "\n",
    "third_stage = pd.read_csv(r'\\1Hz.csv')\n",
    "\n",
    "# find the word with low frequency: time = 1\n",
    "\n",
    "all_words = third_stage['words'].values\n",
    "#print(len(np.unique(all_words)))\n",
    "low_frequency_words = []\n",
    "words_id = []\n",
    "for i in range(len(all_words)):\n",
    "    if list(all_words).count(all_words[i])<=1:\n",
    "        words_id.append(all_words[i])\n",
    "        print(third_stage.loc[i]['id'],third_stage.loc[i]['t'],all_words[i],list(all_words).count(all_words[i]))\n",
    "        low_frequency_words.append([third_stage.loc[i]['id'],third_stage.loc[i]['t'],all_words[i],i])\n",
    "#list(all_words).count(688)\n",
    "\n",
    "unique_word = list(set(words_id))\n",
    "print(unique_word,len(unique_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data: remove the word id with low frequency\n",
    "\n",
    "clean_data = third_stage\n",
    "origin_words = third_stage['words'].values\n",
    "for i in range(len(low_frequency_words)):\n",
    "    temp_id = low_frequency_words[i][0]\n",
    "    temp_t = low_frequency_words[i][1]\n",
    "    temp_i = low_frequency_words[i][-1]\n",
    "    print(temp_id,temp_t,temp_i)\n",
    "    \n",
    "    if temp_t-1>0 and clean_data.loc[temp_i-1]['words'] not in unique_word \\\n",
    "        and clean_data.loc[temp_i-1]['id'] == temp_id and clean_data.loc[temp_i-1]['t'] == clean_data.loc[temp_i]['t']-1:\n",
    "        origin_words[temp_i]=clean_data.loc[temp_i-1]['words']\n",
    "    elif temp_t-2>0 and clean_data.loc[temp_i-2]['words'] not in unique_word \\\n",
    "        and clean_data.loc[temp_i-2]['id'] == temp_id and clean_data.loc[temp_i-2]['t'] == clean_data.loc[temp_i]['t']-2:\n",
    "        origin_words[temp_i]=clean_data.loc[temp_i-2]['words']\n",
    "    else:\n",
    "        origin_words[temp_i]=clean_data.loc[temp_i+1]['words']\n",
    "\n",
    "clean_data['words'] = pd.DataFrame(np.array(origin_words))\n",
    "print(clean_data)\n",
    "\n",
    "clean_data.to_csv(r'\\clean_1Hz(new).csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-scene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def texts_to_corpus(temp_text):\n",
    "    uni_words = np.unique(np.array(temp_text))\n",
    "    corpus = []\n",
    "    for uni_word in uni_words:\n",
    "        corpus.append((uni_word,temp_text.count(uni_word)))\n",
    "    return corpus\n",
    "\n",
    "def test2str(t_texts):\n",
    "    str_text = []\n",
    "    for t_text in t_texts:\n",
    "        str_text.append(str(t_text))\n",
    "    return str_text\n",
    "\n",
    "third_stage = pd.read_csv(r'\\clean_1Hz(new).csv')\n",
    "\n",
    "#the texts are represented by each driver\n",
    "\n",
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "\"\"\n",
    "texts = []\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    temp_words = single_data['words'].values\n",
    "    #print(temp_words)\n",
    "    temp_text = []\n",
    "    for temp_word in temp_words:\n",
    "        temp_text.append(str(temp_word))\n",
    "    #print(temp_text)\n",
    "    texts.append(temp_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the dictionary, keep the words with high frequency\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "#import jieba.posseg as jp, jieba\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#dictionary.filter_n_most_frequent(7)\n",
    "corpus = [dictionary.doc2bow(words) for words in texts]\n",
    "# dictionary = corpora.Dictionary.load(r'D:\\科研\\SNN\\new_SNN_0222\\dic_new.dict')\n",
    "# corpus = [dictionary.doc2bow(words) for words in texts]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "\n",
    "dictionary.save(r'\\dic_new.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tfidf to process the corpus\n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "print(len(corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import models, corpora\n",
    "\n",
    "ldamodel = LdaModel(corpus_tfidf,\n",
    "            chunksize=2000,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            decay=0.8,\n",
    "            iterations=1200,\n",
    "            num_topics=5,\n",
    "            passes=20,\n",
    "            eval_every=1)\n",
    "\n",
    "temp_file = datapath(r\"\\auto_model_new_test.model\")\n",
    "ldamodel.save(temp_file)\n",
    "\n",
    "# ldamodel = models.ldamodel.LdaModel.load(r\"D:\\科研\\SNN\\new_SNN_0222\\auto_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the effectiveness of ldamodel\n",
    "\n",
    "test_cor = dictionary.doc2bow(['683','528','501','714','497','714','683'])\n",
    "#test_cor = [words_to_cor(683),words_to_cor(528),words_to_cor(501),words_to_cor(714),words_to_cor(497),words_to_cor(714),words_to_cor(683)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['657','831','840','863','687','869','807'])\n",
    "#test_cor = [words_to_cor(657),words_to_cor(831),words_to_cor(840),words_to_cor(863),words_to_cor(687),words_to_cor(869),words_to_cor(807)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['499','683','714','684','558','528','528'])\n",
    "#test_cor = [words_to_cor(499),words_to_cor(683),words_to_cor(714),words_to_cor(684),words_to_cor(558),words_to_cor(528),words_to_cor(528)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "test_cor = dictionary.doc2bow(['373','404','404','373','404','373','372'])\n",
    "#test_cor = [words_to_cor(373),words_to_cor(404),words_to_cor(404),words_to_cor(373),words_to_cor(404),words_to_cor(373),words_to_cor(372)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])\n",
    "\n",
    "#test_cor = [(528, 6), (558, 1)]\n",
    "test_cor = dictionary.doc2bow(['528','528','528','528','528','528','558'])\n",
    "#test_cor = [words_to_cor(373),words_to_cor(404),words_to_cor(404),words_to_cor(373),words_to_cor(404),words_to_cor(373),words_to_cor(372)]\n",
    "print(test_cor,ldamodel[tfidf_model[test_cor]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-maryland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_known(prediction):\n",
    "    known_topics = []\n",
    "    knowm_fea = []\n",
    "    for i in range(len(prediction)):\n",
    "        known_topics.append(prediction[i][0])\n",
    "        knowm_fea.append(prediction[i][1])\n",
    "    return known_topics,knowm_fea\n",
    "\n",
    "def construct_feature(prediction,num_topic):\n",
    "    known_topics,knowm_fea = search_known(prediction)\n",
    "    #print(known_topics)\n",
    "    temp_feature = []\n",
    "    if len(known_topics)==num_topic:\n",
    "        return knowm_fea\n",
    "    for i in range(num_topic):\n",
    "        completion = (1-sum(knowm_fea))/(num_topic-len(known_topics))\n",
    "        if i in known_topics:\n",
    "            indexs = known_topics.index(i)\n",
    "            temp_feature.append(knowm_fea[indexs])\n",
    "        else:\n",
    "            temp_feature.append(completion)\n",
    "    return temp_feature\n",
    "\n",
    "def test2str(t_texts):\n",
    "    str_text = []\n",
    "    for t_text in t_texts:\n",
    "        str_text.append(str(t_text))\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "total_times = 0\n",
    "num_topic = 5\n",
    "train_data = []\n",
    "single_length = []\n",
    "\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    for i in range(len(single_data)-6):\n",
    "        #for j in range(i,i+7):\n",
    "        temp_text = [single_data.loc[i]['words'],single_data.loc[i+1]['words'],single_data.loc[i+2]['words'],\n",
    "                    single_data.loc[i+3]['words'],single_data.loc[i+4]['words'],single_data.loc[i+5]['words'],\n",
    "                    single_data.loc[i+6]['words']]\n",
    "        \n",
    "        tuple_corpus = dictionary.doc2bow(test2str(temp_text))\n",
    "        temp_feature = construct_feature(ldamodel[tfidf_model[tuple_corpus]],num_topic)\n",
    "        ID = single_data.loc[i+6]['id']\n",
    "        t = single_data.loc[i+6]['t']\n",
    "        corpus = dictionary.doc2bow([str(single_data.loc[i+6]['words'])])\n",
    "#         his_text = [single_data.loc[i+1]['words'],single_data.loc[i+2]['words'],\n",
    "#                     single_data.loc[i+3]['words'],single_data.loc[i+4]['words'],single_data.loc[i+5]['words'],\n",
    "#                     single_data.loc[i+6]['words']]\n",
    "        train_data.append([ID,t,len(single_data)-6,corpus,temp_feature,int(single_data.loc[i+6]['words'])])\n",
    "    total_times += len(single_data)-6\n",
    "    single_length.append(len(single_data)-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(temp_train):\n",
    "    np_train = np.array(temp_train)\n",
    "    if np_train.shape[0]>15:\n",
    "        print('error')\n",
    "        return np_train\n",
    "    elif np_train.shape[0]==15:\n",
    "        return np_train\n",
    "    else:\n",
    "        loss_len = 15-np_train.shape[0]\n",
    "        loss_train = np.zeros([loss_len,5])\n",
    "        return np.concatenate([loss_train,np_train],axis=0)\n",
    "    \n",
    "ids = np.unique(third_stage['id'].to_numpy())\n",
    "flag = 0\n",
    "minimum = 4\n",
    "maximum = 14\n",
    "all_train = []\n",
    "train_speed = []\n",
    "target_speed = []\n",
    "np_all_train = np.zeros([0,15,5])\n",
    "\n",
    "for ID in ids:\n",
    "    print(ID)\n",
    "    single_data = third_stage[third_stage['id']==ID].reset_index().drop(columns = ['index'])\n",
    "    length = single_length[int(ID)-1]\n",
    "    temp_data = train_data[flag:flag+length]\n",
    "    flag += length\n",
    "    if length<4+1:\n",
    "        continue\n",
    "    for i in range(length-minimum+1):\n",
    "        end_num = min(length-1,i+maximum)\n",
    "        #print(i,end_num)\n",
    "        for j in range(i+minimum,end_num+1):\n",
    "            next_speed = temp_data[j][5]\n",
    "            current_speed = temp_data[j-1][5]\n",
    "            train_speed.append(next_speed)\n",
    "            target_speed.append(current_speed)\n",
    "            temp_train = []\n",
    "            for k in range(i,j+1):\n",
    "                temp_train.append(temp_data[k][4])\n",
    "            np_temp_train = pad_seq(temp_train).reshape([-1,pad_seq(temp_train).shape[0],pad_seq(temp_train).shape[1]])\n",
    "            all_train.append(temp_train)\n",
    "            np_all_train = np.concatenate([np_all_train,np_temp_train],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_speed = np.array(train_speed).reshape(-1,1)\n",
    "np_target_speed = np.array(target_speed).reshape(-1,1)\n",
    "print(np_all_train.shape,np_train_speed.shape,np_target_speed.shape)\n",
    "\n",
    "np.save(r'\\np_all_train_new.npy',np_all_train)\n",
    "np.save(r'\\np_train_speed_new.npy',np_train_speed)\n",
    "np.save(r'\\np_target_speed_new.npy',np_target_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-possible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
